{"cells":[{"cell_type":"markdown","source":["### FUNCIONES TEST\n\n- ``row_counting_deviation(df, previous_df, deviation_threshold = 0.1)``: Test que chequea si un dataframe tiene una desviacion en el numero de filas mayor a lo indicado en deviation_threshold con respecto a otro dataframe previo\n- ``column_counting_deviation(df, previous_df, deviation_threshold = 0.0)``: Test que chequea si un dataframe tiene una desviacion en el numero de columnas mayor a lo indicado en deviation_threshold con respecto a otro dataframe previo\n- ``column_names_checking(df, previous_df)``: Test que chequea si los nombres de las columnas de un dataframe son iguales a las del dataframe previo\n- ``duplicated_key(df, columns_list)``: Test que chequea si hay duplicados en la clave unica de un dataframe\n- ``nulls_check(df, columns_list)``: Test que chequea si hay valores nulos en una serie de columnas\n- ``sum_column_values_deviation(df, previous_df, columns_list, deviation_threshold = 0.1)``: Test que chequea si la suma del total de los valores de una serie de columnas se desvia en mas de deviation_threshold con respecto a un dataframe previo\n- ``sum_column_aggregated_values_deviation(df, previous_df, columns_list, aggregation_keys, deviation_threshold = 0.1)``: Test que chequea si la suma agregada por variables dadas de los valores de una serie de columnas se desvia en mas de deviation_threshold con respecto a un dataframe previo\n- ``categories_counting_deviation(df, previous_df, columns_list, deviation_threshold = 0.1)``: Test que imprime la distribucion de las variables definidas en proporcion de sus categorias. Si se indica un previous_df ademas comprobara diferencias en las categorias con la distribucion de ese df anterior. Si se supera el umbral dado generara un error\n- ``range_control(df, columns_list, range_list)``: Test que chequea si los valores de una serie de columnas estan entre un rango dado, en caso de variables categoricas, chequea que todos los valores de la lista dada estan en la variable del dataframe, y no hay ninguna categoria extra en el\n- ``run_all_tests(df, dict_argumentos)``: Ejecuta todos los tests que indiquemos en test_executions de dict_argumentos de manera automatica. Debemos darle los valores de entrada de los tests en forma de diccionarios con el nombre del test como clave en string y los valores como valor. \n- ``total_columns_sum_check(df, dict_sum_of_columns):``: Test que comprueba si la columna que deberia equivaler a la suma de diferentes columnas esta bien calculado. \n- ``variables_type(df, columns_list, types_list)``: Test que chequea si el formato de una lista de variables coincide con el que nosotros especifiquemos \n- ``variable_distribution(df, previous_df, columns_list, deviation_threshold = 0.1)``: Test que chequea si hay desviaciones en los p0 p25 p40 p50 p60 p75 p100 y la media de distribucion de un df con uno previo. Si no se da un datarame previo se pintara la distribucion del actual\n- ``criteria_pointed_check(df, specifications)``: Test que chequea si la puntuacion de los agentes esta bien calculada con respecto a las condiciones que le corresponderian a cada uno de esos agentes.\n- ``round_numeric_vars(df, exclude_cols = [], scale = 2)``: Función que redondea los valores numéricos decimales de un DataFrame al tipo DecimalType(18, scale). Las columnas incluidas en el parámetro de entrada 'exclude_cols' no son transformadas independientemente de su tipo inicial.\n- ``check_equal_dfs(df1, df2, exclude_conv_cols=[], scale_num_conv=2)``: Test que chequea si dos DataFrames son iguales. Las columnas numéricas decimales de ambos DataFrames son transformadas de acuerdo con la función ``round_numeric_vars``.\n- ``saving_after_testing(df, environment, output_table_name, unique_key='cod_mediador', carga = 'overwrite')``: Funcion que define la ruta de guardado de un proceso dependiendo del tipo de testeo que se este ejecutando.\n- ``saving_after_testing_dlake(df, environment, output_table_name, table_format, partitioned_by=None, replace_where=None, container=None, carga ='overwrite')``: Funcion que define la ruta de guardado de un proceso dependiendo del tipo de testeo que se este ejecutando (entorno de ejecución Data Lake)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc16bd92-a988-45d0-9787-b5bcabadadad"}}},{"cell_type":"code","source":["import datavault as dv\nimport analytics.general_utils as gen_utils\nimport analytics.ml_pipeline as ml_pipe\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nfrom pyspark.sql import Row\nimport os\n\nfrom datetime import datetime\nimport pandas as pd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e48202b8-b7d6-44d5-aff6-8b55eef78a94"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def row_counting_deviation(df, previous_df, deviation_threshold = 0.1):\n  \n  \"\"\"\n  Test que chequea si un dataframe tiene una desviacion en el numero de filas mayor a lo indicado en deviation_threshold con respecto a otro dataframe previo\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  previous_df : Spark DataFrame\n    DataFrame previo contra el que queremos comprobar\n  deviation_threshold : Float\n    Desviacion en decimal\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si la desviacion es mayor. Mensaje de OK si pasa el test \n  \"\"\"\n\n  current_df_count = df.count()\n  previous_df_count = previous_df.count()\n  deviation = abs((current_df_count - previous_df_count) / previous_df_count)\n  if deviation >= deviation_threshold:\n    print('{0: .2f}'.format(deviation * 100) + '%')\n    raise SystemExit('Hay una desviacion igual o mayor al {}% en el numero de filas de la nueva tabla con respecto a la version previa'.format(deviation_threshold * 100))\n  \n  else:\n    return 'Desviacion de numero de filas dentro de limites OK, desviacion del {0:.0f}%'.format(deviation * 100)\n  \n#---------------------------------------------------------------------------------\n\ndef column_counting_deviation(df, previous_df, deviation_threshold = 0.0):\n  \n  \"\"\"\n  Test que chequea si un dataframe tiene una desviacion en el numero de columnas mayor a lo indicado en deviation_threshold con respecto a otro dataframe previo\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  previous_df : Spark DataFrame\n    DataFrame previo contra el que queremos comprobar\n  deviation_threshold : Float\n    Desviacion en decimal\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si la desviacion es mayor. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  current_df_count = 0\n  previous_df_count = 0\n  \n  for column in df.columns:\n    current_df_count += 1\n    \n  for column in previous_df.columns:\n    previous_df_count += 1\n    \n  deviation = abs((current_df_count - previous_df_count) / previous_df_count)\n  \n  if deviation > 0:\n    \n    current_df_cols = [col.upper() for col in df.columns]\n    previous_df_cols = [col.upper() for col in previous_df.columns]\n\n    lost_cols = [col for col in previous_df_cols if col not in current_df_cols]\n    new_cols = [col for col in current_df_cols if col not in previous_df_cols]\n        \n    if deviation > deviation_threshold:\n      raise SystemExit('Hay una desviacion igual o mayor al {0:.0f}% en el numero de columnas de la nueva tabla con respecto a la version previa. Las columnas {1} son nuevas y las columnas {2} se han eliminado.'.format(deviation_threshold * 100, new_cols, lost_cols))\n\n    else:\n      return 'Desviacion en numero de columnas dentro de limites OK, desviacion del {0:.0f}%. Las columnas {1} son nuevas y las columnas {2} se han eliminado.'.format(deviation * 100, new_cols, lost_cols)\n    \n  else:\n    return 'Desviacion en numero de columnas OK, desviacion del {0:.0f}%'.format(deviation * 100)\n\n#---------------------------------------------------------------------------------\n\ndef column_names_checking(df, previous_df):\n  \n  \"\"\"\n  Test que chequea si los nombres de las columnas de un dataframe son iguales a las del dataframe previo\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  previous_df : Spark DataFrame\n    DataFrame previo contra el que queremos comprobar\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay columnas con nombres diferentes. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  current_df_cols = []\n  previous_df_cols = []\n  new_cols = []\n  lost_cols = []\n  \n  current_df_cols = [col.upper() for col in df.columns]\n  previous_df_cols = [col.upper() for col in previous_df.columns]\n    \n  lost_cols = [col for col in previous_df_cols if col not in current_df_cols]\n  new_cols = [col for col in current_df_cols if col not in previous_df_cols]\n      \n  if len(lost_cols) > 0:\n    raise SystemExit('Las columnas {} no estan incluidas en la nueva tabla'.format(lost_cols))\n      \n  elif len(new_cols) > 0:\n    raise SystemExit('La columnas {} no estaban incluidas en la tabla anterior'.format(new_cols))\n  \n  else:\n    return 'Nombres de columnas sin modificaciones OK'\n\n#---------------------------------------------------------------------------------\n\ndef duplicated_key(df, columns_list):\n  \n  \"\"\"\n  Test que chequea si hay duplicados en la clave unica de un dataframe\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  columns_list : String / list of strings\n    Lista con las columnas que crean la clave unica\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay duplicados. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  if type(columns_list) is str:\n    columns_list = [columns_list]\n  \n  grouped_data = df.groupBy(columns_list).count()\n  duplicated_rows = grouped_data.where(F.col('count') > 1).count()\n  \n  if duplicated_rows > 0:\n    grouped_data.orderBy('count', ascending = False).show()\n    raise SystemExit('Hay valores de la key proporcionada en row_list duplicados. Se muestran ejemplos en la tabla.')\n\n  else:\n    return 'No duplicados en la clave unica OK'\n  \n#---------------------------------------------------------------------------------\n\ndef nulls_check(df, columns_list = None):\n  \n  \"\"\"\n  Test que chequea si hay valores nulos en una serie de columnas\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  columns_list : String / list of strings\n    Lista con las columnas que se quieren testear\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay nulos. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  if columns_list == None:\n    columns_list = [col for col in df.columns]\n  \n  elif type(columns_list) is str:\n    columns_list = [columns_list]\n  \n  columns_with_nulls = []\n  \n  for column in columns_list:\n    if df.select(column).where(F.col(column).isNull()).count() > 0:\n      columns_with_nulls.append(column)\n  \n    if len(columns_with_nulls) > 0:\n      raise SystemExit('Hay valores nulos en la columna {}'.format(column))\n  \n  \n  return 'No nulos OK'\n\n#---------------------------------------------------------------------------------\n\ndef sum_column_values_deviation(df, previous_df, columns_list, deviation_threshold = 0.1, nivel_mediador = False):\n  \n  \"\"\"\n  Test que chequea si la suma del total de los valores de una serie de columnas se desvia en mas de deviation_threshold con respecto a un dataframe previo\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  previous_df : Spark DataFrame\n    DataFrame anterior contra el que queremos comparar\n  columns_list : String / list of strings\n    Lista con las columnas que se quieren testear\n  deviation_threshold : Float\n    Desviacion en decimal\n  nivel_mediador : boolean\n    Si esta en True, generara ademas un test a nivel mediador de las columnas incluidas. Por defecto False\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay alguna columna con una desviacion mayor a deviation_threshold, muestra la columna que se ha desviado. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  previous_df = gen_utils.convert_numeric_variables_to_float(previous_df)\n  df = gen_utils.convert_numeric_variables_to_float(df)\n  \n  if type(columns_list) is str:\n    columns_list = [columns_list]\n    \n  for column in columns_list:\n    sum_current_column_values = df.select(column).groupBy().agg(F.sum(column)).collect()[0][0]\n    sum_previous_column_values = previous_df.select(column).groupBy().agg(F.sum(column)).collect()[0][0]\n    deviation = abs((sum_current_column_values - sum_previous_column_values) / sum_previous_column_values)\n    if deviation > deviation_threshold:\n      raise SystemExit('Hay una desviacion de mas del {0:.0f}% en la suma de valores de la columna {1}'.format(deviation_threshold * 100, column))\n    \n    elif deviation > 0:\n      print('Desviacion del {0:.0f}% en la suma de valores de la columna {1} inferior al limite. Valor anterior {2:.2f}, valor actual {3:.2f}.'.format(deviation * 100, column, sum_previous_column_values, sum_current_column_values))\n    \n    if nivel_mediador == True:\n      sum_current_column_values = df.groupBy('cod_mediador').agg(F.sum(column)).withColumnRenamed('sum(' + column + ')',  column + '_current')\n      sum_previous_column_values = previous_df.groupBy('cod_mediador').agg(F.sum(column)).withColumnRenamed('sum(' + column + ')', column + '_prev')\n      joined_values = sum_current_column_values.join(sum_previous_column_values, how = 'full', on = 'cod_mediador').na.fill(0)\\\n                                               .withColumn('diff', F.udf(lambda x, y: 0 if x == 0 else abs((float(x)- float(y)) / float(x)), T.FloatType())(column + '_current', column + '_prev'))\n      \n      if joined_values.where(F.col('diff') > deviation_threshold).count() > 0 :\n        joined_values.where(F.col('diff') > deviation_threshold).show()\n        raise SystemExit('Hay una desviacion de mas del {0:.0f}% en la columna {1} en {2} mediadores. Se muestran casos.'.format(deviation_threshold * 100, column, joined_values.where(F.col('diff') > deviation_threshold).count()))\n\n      elif deviation > 0:\n        joined_values.where((F.col('diff') > 0) & (F.col('diff') <= deviation_threshold)).show()\n        print('Hay una desviacion menor al {0:.0f}% en la columna {1} en {2} mediadores. Se muestran casos.'.format(deviation_threshold * 100, column, joined_values.where((F.col('diff') > 0) & (F.col('diff') <= deviation_threshold)).count()))\n      \n      else:\n        print('Desviacion en columna {0} para todos los mediadores OK'.format(column))\n\n  return 'Desviacion de suma de valores de columnas OK'\n\n#---------------------------------------------------------------------------------\n\ndef sum_column_aggregated_values_deviation(df, previous_df, columns_list, aggregation_keys, deviation_threshold = 0.1):\n  \n  \"\"\"\n  Test que chequea si la suma agregada por variables dadas de los valores de una serie de columnas se desvia en mas de deviation_threshold con respecto a un dataframe previo\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  previous_df : Spark DataFrame\n    DataFrame anterior contra el que queremos comparar\n  columns_list : String / list of strings\n    Lista con las columnas que se quieren testear\n  aggregation_keys : String / list of strings\n    Las variables por las que queremos que agrupe la suma\n  deviation_threshold : Float\n    Desviacion en decimal\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay alguna columna con una desviacion mayor a deviation_threshold, muestra una tabla con los valores que mas se devian. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  previous_df = gen_utils.convert_numeric_variables_to_float(previous_df)\n  df = gen_utils.convert_numeric_variables_to_float(df)\n  \n  if type(columns_list) is str:\n    columns_list = [columns_list]\n  \n  for column in columns_list:\n    sum_current_column_values = df.groupBy(aggregation_keys).agg(F.sum(column).alias(column + '_current'))\n    sum_previous_column_values = previous_df.groupBy(aggregation_keys).agg(F.sum(column).alias(column + '_prev'))\n    \n    joined_values = sum_previous_column_values.join(sum_current_column_values, on = aggregation_keys, how = 'full').na.fill(0)\\\n                                             .withColumn('diff', F.udf(lambda x, y: 0 if x == 0 else abs((float(y) - float(x)) / float(x)), T.FloatType())(column + '_prev', column + '_current'))\n    \n    deviated_rows = joined_values.where(F.col('diff') > deviation_threshold)\n    \n    if deviated_rows.count() > 0:\n      deviated_rows.orderBy('diff', ascending = False).show()\n      raise SystemExit('Hay una desviacion de mas del {0:.0f}% para la columna {1} en las agregaciones mostradas en la tabla'.format(deviation_threshold * 100, column))\n    elif joined_values.where(F.col('diff') > 0).count() > 0:\n      joined_values.where(F.col('diff') > 0).orderBy('diff', ascending = False).show()\n      print('Hay desviaciones en suma de valores agregados que son inferiores al limite {0:.0f}% en la columna {1}. En la tabla se muestran algunos casos. Desviacion en suma de valores agregados de columnas dentro de limites OK'.format(deviation_threshold * 100, column))\n  \n  return 'Desviacion en suma de valores agregados de OK.'\n\n#---------------------------------------------------------------------------------\n\ndef categories_counting_deviation(df, previous_df, columns_list, deviation_threshold = 0.1):\n  \n  \"\"\"\n  Test que imprime la distribucion de las variables definidas en proporcion de sus categorias. Si se indica un previous_df ademas comprobara diferencias en las categorias con la distribucion de ese df anterior. Si se supera el umbral dado generara un error\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  previous_df : Spark DataFrame\n    DataFrame anterior contra el que queremos comparar\n  columns_list : String / list of strings\n    Lista con las columnas que se quieren testear\n  deviation_threshold : Float\n    Desviacion en decimal\n  \n  Returns:\n  --------\n  Distribucion de las categorioas del df actal. Si hay previous_df mensaje de error y la ejecucion se para si hay alguna columna con una desviacion mayor a deviation_threshold, muestra una tabla con los valores que mas se devian. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  if type(columns_list) is str:\n      columns_list = [columns_list]\n  \n  if previous_df == None:\n    df = gen_utils.convert_numeric_variables_to_float(df)\n\n    for column in columns_list:\n      current_category_count = df.groupBy(column).count().withColumnRenamed('count', column + '_current_count')\n      current_total_count = df.groupBy().count().collect()[0][0]\n      current_category_count = current_category_count.withColumn('total_count', F.lit(current_total_count))\\\n                                                     .withColumn('category_per_current', F.udf(lambda x, y: (float(x) / float(y) * 100) , T.FloatType())(column + '_current_count', 'total_count'))\\\n                                                     .drop('total_count')  \n\n      print('Distribucion de la variable {}:'.format(column))\n      current_category_count.show()\n  \n  elif previous_df != None:\n    previous_df = gen_utils.convert_numeric_variables_to_float(previous_df)\n\n    for column in columns_list:\n      current_category_count = df.groupBy(column).count().withColumnRenamed('count', column + '_current_count')\n      current_total_count = df.groupBy().count().collect()[0][0]\n      current_category_count = current_category_count.withColumn('total_count', F.lit(current_total_count))\\\n                                                     .withColumn('category_per_current', F.udf(lambda x, y: (float(x) / float(y) * 100) , T.FloatType())(column + '_current_count', 'total_count'))\\\n                                                     .drop('total_count')\n      \n      print('Distribucion de la variable {}:'.format(column))\n      current_category_count.show()\n      \n      previous_category_count = previous_df.groupBy(column).count().withColumnRenamed('count', column + '_prev_count')\n      previous_total_count = previous_df.groupBy().count().collect()[0][0]\n      previous_category_count = previous_category_count.withColumn('total_count', F.lit(previous_total_count))\\\n                                                       .withColumn('category_per_prev', F.udf(lambda x, y: (float(x) / float(y) * 100) , T.FloatType())(column + '_prev_count','total_count'))\\\n                                                       .drop('total_count')\n\n      joined_values = previous_category_count.join(current_category_count, how = 'full', on = column).na.fill(0)\\\n                                            .withColumn('diff', F.udf(lambda x, y: 0 if x == 0 else abs((float(y) - float(x)) / float(x)), T.FloatType())('category_per_prev', 'category_per_current'))\n\n      deviated_rows = joined_values.where(F.col('diff') > deviation_threshold)\n\n      if deviated_rows.count() > 0:\n        deviated_rows.orderBy('diff', ascending = False).show()\n        raise SystemExit('Hay una desviacion de mas del {0:.0f}% para la columna {1} en las categorias mostradas en la tabla'.format(deviation_threshold * 100, column))\n      elif joined_values.where(F.col('diff') > 0).count() > 0: \n        print('Diferencias en la distribucion de la variable {}:'.format(column))\n        joined_values.where(F.col('diff') > 0).orderBy('diff', ascending = False).show()\n        print('Hay desviaciones en distribucion de la columna {0} que son inferiores al limite {1:.0f}%. En la tabla se muestran algunos casos.'.format(column, deviation_threshold * 100))\n\n  return 'Distribuciones por categoria dentro de limites OK'\n\n#---------------------------------------------------------------------------------\n\ndef range_control(df, columns_list, range_list):\n  \n  \"\"\"\n  Test que chequea si los valores de una serie de columnas estan entre un rango dado, en caso de variables categoricas, chequea que todos los valores de la lista dada estan en la variable del dataframe, y no hay ninguna categoria extra en el\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  columns_list : String / list of strings\n    Lista con las columnas que se quieren testear\n  range_list : list of floats / list of strings / list of lists\n    Lista con el o los rangos de las columnas que queremos testear. En caso de variables numericas se indicaran los valores maximos y minimos, en caso de categoricas se indicaran todas las categorias que la variable debe tener\n\n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay alguna columna que salga del rango o que tenga categorias diferentes a las indicadas, muestra una tabla con los valores que mas se devian. Mensaje de OK si pasa el test\n  \"\"\"\n    \n  if type(columns_list) is str:\n    columns_list = [columns_list]\n  \n  if type(range_list[0]) is not list:\n    range_list = [range_list]\n    \n  counter = 0\n  \n  for column in columns_list:\n    if type(range_list[counter][0]) is str:\n      elements_in_df = df.select(column).dropDuplicates().toPandas()\n      for element in range_list[counter]:\n        if element in set(elements_in_df[column]) ==  False:\n          raise SystemExit('El valor {} no esta en contenido en la columna {}'.format(element, column))\n\n      for element in set(elements_in_df[column]):\n        if element in range_element ==  False:\n          raise SystemExit('El valor {} esta en contenido en la columna {} y no es parte del rango de valores definidos'.format(element, column))\n\n    elif type(range_list[counter][0]) is not str:\n      max_bound = max(range_list[counter])\n      min_bound = min(range_list[counter])\n      outbounded_values = df.where((F.col(column) < min_bound) | (F.col(column) > max_bound))\n      if outbounded_values.count() > 0:\n        raise SystemExit('Hay valores fuera de rango en la columna {}'.format(column))\n\n    counter += 1\n\n  return 'Control de rangos y valores OK'\n\n#---------------------------------------------------------------------------------\n\ndef run_all_tests(df, dict_argumentos):\n  \n  \"\"\"\n  Ejecuta todos los tests que indiquemos en test_executions de dict_argumentos de manera automatica. Debemos darle los valores de entrada de los tests en forma de diccionarios con el nombre del test como clave en string y los valores como valor. \n  \n  Ejemplo:\n  \n  dict_argumentos = {'tests': tests, \n                   'previous_df': df_comparacion,\n                   'dict_args_columns_lists': argumentos_columnas,\n                   'dict_ranges': argumentos_rangos,\n                   'dict_thresholds': argumentos_desviaciones,\n                   'dict_total_sum': argumentos_columna_suma_total,\n                   'dict_specifications': dict_especificaciones}\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  dict_argumentos : dict of dicts\n    Lista con los diccionarios que queremos meter en la funcion\n\n  Returns:\n  --------\n  Devuelve la salida de cada uno de los tests que se ejecutan. Si alguno falla, la ejecucion se para\n  \"\"\"\n  \n  if 'tests' in dict_argumentos.keys():\n    test_executions = dict_argumentos['tests']\n  else:\n    raise SystemExit('No se ha especificado ningun test')\n  \n  if 'previous_df' in dict_argumentos.keys():\n    previous_df = dict_argumentos['previous_df']\n  else:\n    previous_df = None\n  if 'dict_args_columns_lists' in dict_argumentos.keys():\n    dict_args_columns_lists = dict_argumentos['dict_args_columns_lists']\n  if 'dict_args_aggs' in dict_argumentos.keys():\n    dict_args_aggs = dict_argumentos['dict_args_aggs']\n  if 'dict_ranges' in dict_argumentos.keys():\n    dict_ranges = dict_argumentos['dict_ranges']\n  if 'dict_thresholds' in dict_argumentos.keys():\n    dict_thresholds = dict_argumentos['dict_thresholds']\n  else:\n    dict_thresholds = {}\n  if 'dict_types' in dict_argumentos.keys():\n    dict_types = dict_argumentos['dict_types']\n  if 'dict_total_sum' in dict_argumentos.keys():\n    dict_total_sum = dict_argumentos['dict_total_sum']\n  if 'dict_specifications' in dict_argumentos.keys():\n    dict_specifications = dict_argumentos['dict_specifications']\n  if 'analisis_mediadores' in dict_argumentos.keys():\n    analisis_mediadores = dict_argumentos['analisis_mediadores']\n  else:\n    analisis_mediadores = False\n    \n  if 'row_counting_deviation' in test_executions:\n    if 'row_counting_deviation' in dict_thresholds:\n      print(row_counting_deviation(df, previous_df, dict_thresholds['row_counting_deviation']))\n    else:   \n      print(row_counting_deviation(df, previous_df))\n  \n  if 'column_counting_deviation' in test_executions:\n    if 'column_counting_deviation' in dict_thresholds:\n      print(column_counting_deviation(df, previous_df, dict_thresholds['column_counting_deviation']))\n    else:\n      print(column_counting_deviation(df, previous_df))\n  \n  if 'column_names_checking' in test_executions:\n    print(column_names_checking(df, previous_df))\n    \n  if 'duplicated_key' in test_executions:\n    print(duplicated_key(df, dict_args_columns_lists['duplicated_key']))\n  \n  if 'nulls_check' in test_executions and 'nulls_check' in dict_args_columns_lists:\n    print(nulls_check(df, dict_args_columns_lists['nulls_check']))\n  elif 'nulls_check' in test_executions:\n    print(nulls_check(df))\n    \n  if 'sum_column_values_deviation' in test_executions:\n    if 'sum_column_values_deviation' in dict_thresholds:\n      print(sum_column_values_deviation(df, previous_df, dict_args_columns_lists['sum_column_values_deviation'], dict_thresholds['sum_column_values_deviation'], nivel_mediador = analisis_mediadores))\n    else:\n      print(sum_column_values_deviation(df, previous_df, dict_args_columns_lists['sum_column_values_deviation']), nivel_mediador = analisis_mediadores)\n  \n  if 'sum_column_aggregated_values_deviation' in test_executions:\n    if 'sum_column_aggregated_values_deviation' in dict_thresholds:\n      print(sum_column_aggregated_values_deviation(df, previous_df, dict_args_columns_lists['sum_column_aggregated_values_deviation'], dict_args_aggs['sum_column_aggregated_values_deviation'], dict_thresholds['sum_column_aggregated_values_deviation']))\n    else:\n      print(sum_column_aggregated_values_deviation(df, previous_df, dict_args_columns_lists['sum_column_aggregated_values_deviation'], dict_args_aggs['sum_column_aggregated_values_deviation']))\n  \n  if 'categories_counting_deviation' in test_executions:\n    if 'categories_counting_deviation' in dict_thresholds:\n      print(categories_counting_deviation(df, previous_df, dict_args_columns_lists['categories_counting_deviation'], dict_thresholds['categories_counting_deviation']))\n    else:\n      print(categories_counting_deviation(df, previous_df, dict_args_columns_lists['categories_counting_deviation']))\n    \n  if 'range_control' in test_executions:\n    print(range_control(df, dict_args_columns_lists['range_control'], dict_ranges['range_control']))\n    \n  if 'variables_type' in test_executions:\n    print(variables_type(df, dict_args_columns_lists['variables_type'], dict_types['variables_type']))\n  \n  if 'total_columns_sum_check' in test_executions:\n    print(total_columns_sum_check(df, dict_total_sum))\n       \n  if 'variable_distribution' in test_executions:\n    if 'variable_distribution' in dict_thresholds:\n      print(variable_distribution(df, previous_df, dict_args_columns_lists['variable_distribution'], dict_thresholds['variable_distribution']))\n    else:\n      print(variable_distribution(df, previous_df, dict_args_columns_lists['variable_distribution']))\n      \n  if 'criteria_pointed_check' in test_executions:\n    print(criteria_pointed_check(df, dict_specifications['criteria_pointed_check']))\n  \n  if 'criteria_pointed_check_planexcelencia' in test_executions:\n    print(criteria_pointed_check_planexcelencia(df, dict_specifications['criteria_pointed_check_planexcelencia']))\n    \n\n#---------------------------------------------------------------------------------\n    \ndef saving_after_testing(df, environment, output_table_name, unique_key='cod_mediador', carga = 'overwrite'):\n  \n  \"\"\"\n  Funcion que define la ruta de guardado de un proceso dependiendo del tipo de testeo que se este ejecutando\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    Dataframe resultado del proceso\n  environment : String\n    Es el entorno en el que queremos testear el proceso. Puede ser desarrollo para desarrollar, tester para comprobar el codigo antes de subir a produccion, produccion para tests generales de procesos productivos automaticos que guardan en staging. Generalmente vendra dado por los widgets del job\n  output_table_name : String\n    Ruta en la que se guardara la salida del proceso. Generalmente vendra dado por los widgets del job \n   \n  Returns:\n  --------\n  Un dataframe que se guardara en la ruta dada, devolvera mensaje SUCCESS. Si el entorno se introduce erroneamente devolvera un mensaje de error\n  \"\"\"\n  \n  # testing_blob_path = '/mnt/datavault/testing/'\n  # testing_blob_path_tester = '/mnt/datavault/test_registros_cambios/'\n  \n  testing_path = 'testing/desarrollo/'\n  testing_path_tester = 'testing/tester/'\n  contianer_dev_test = \"usuarios\"\n  table_format_dev_test = \"parquet\"\n  \n  if environment == 'desarrollo':\n    gen_utils.write_staging_table(df, output_table_name, carga, entorno=\"dev\")\n    \n    return output_table_name\n    \n  elif environment == 'tester':\n    \n    if output_table_name == \"test_registros_cambios\":\n      raise ValueError(\"[ERROR]: La tabla no puede llamarse 'test_registros_cambios'.\")\n  \n    # df.write.mode('overwrite').parquet(testing_blob_path_tester + output_table_name)\n    # return testing_blob_path_tester + output_table_name\n    \n    gen_utils.write_dlake_table(df=df, \n                            table_path=os.path.join(testing_path_tester, output_table_name), \n                            container=contianer_dev_test, \n                            mode=carga, \n                            environment=\"desarrollo\", \n                            table_format=table_format_dev_test)\n    \n    return output_table_name\n    \n  \n  elif environment == 'produccion':\n    #df.write.mode('overwrite').parquet(testing_blob_path + output_table_name)\n    #df = spark.read.parquet(testing_blob_path + output_table_name)\n    \n    #dict_args_columns_lists = {'duplicated_key' : unique_key}\n    #tests = ['duplicated_key', 'nulls_check']\n    \n#     run_all_tests(df, {'tests': tests,\n#                        'dict_args_columns_lists': dict_args_columns_lists})  \n    gen_utils.write_staging_table(df, output_table_name, carga)\n    \n    return \"SUCCESS\"\n\n  else:\n    raise SystemExit('El entorno debe ser desarrollo, tester o produccion')\n    \n#---------------------------------------------------------------------------------\n\ndef variables_type(df, columns_list, types_list):\n  \n  \"\"\"\n  Test que chequea si el formato de una lista de variables coincide con el que nosotros especifiquemos\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  columns_list : String / list of strings\n    Lista con las columnas de las que queremos comprobar el formato\n  types_list : type / list of types\n    Lista con los formatos de spark SQL en formato types de las variables que queremos chequear. Ej: T.StringType, T.FloatType, etc\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay formatos erroneos, nos indicara el error. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  if type(columns_list) is str:\n    columns_list = [columns_list]\n  if type(types_list) is type:\n    types_list = [types_list]\n  \n  for i in range(len(columns_list)):\n    for j in range(len(df.schema.names)):\n      if df.schema.names[j] == columns_list[i]:\n        variable_idx = j\n        variable_real_type = df.schema.fields[variable_idx].dataType\n    \n        if types_list[i]() != variable_real_type:\n          raise SystemExit('La variable {0} no tiene tiene formato {1}. Su formato es {2}'.format(columns_list[i], types_list[i], variable_real_type))\n    \n  return 'Todos los formatos estan OK'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c75f86a-acc9-431a-98b6-2d73f2681582"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def total_columns_sum_check(df, dict_sum_of_columns): \n  \"\"\"\n  Test que comprueba si la columna que deberia equivaler a la suma de diferentes columnas esta bien calculado. \n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  dict_sum_of_columns : dict of lists\n    Diccionario con claves el nombre de la columna que contiene el total a comprobar y valores listas con las columnas que, sumadas, equivalen a ese total\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay sumas mal calculadas, nos indicara en que columna. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  for key, value in dict_sum_of_columns.items():\n    \n    df = df.withColumn('total_sum', sum([df[c] for c in value]))\n    totals = df.select('total_sum', key)\n    totals = totals.withColumn('difference', F.udf(lambda x, y: 0 if y == 0 else abs((float(x) - float(y)) / float(y)), T.FloatType())(key, 'total_sum'))\n\n    deviated_rows = totals.where(F.col('difference') > 0)\n\n    if deviated_rows.count() > 0:\n      deviated_rows.show()\n      raise SystemExit('Hay una desviacion en la suma de la lista de columnas con respecto a la columna total en las filas mostradas')\n    else:\n      print('La suma de valores de columnas con respecto a la columna total {} es correcta'.format(key))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7971b6e7-05ee-496c-aa93-292ec66301f5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def variable_distribution(df, previous_df, columns_list, deviation_threshold = 0.1, distribution_comparation = False):\n  \n  \"\"\"\n  Test que pinta los p0 p25 p40 p50 p60 p75 p100 y la media de distribucion. Si dejamos la distribution_comparation se pintara la distribucion del actual df. Si lo pasamos a True lo comparara con la distribucion de previous_df y usara el threshold para limitar las desviaciones\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  previous_df : Spark DataFrame\n    DataFrame previo\n  columns_list : String / list of strings\n    Lista con las columnas de las que queremos comprobar la distribucion\n  deviation_threshold : Float\n    Desviacion en decimal\n  distribution_comparation : boolean\n    Define si comparamos con un dataframe previo o no\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay desviaciones mayores al threshold, nos indicara en que columna. Si no se da un datarame previo se pintara la distribucion del actual. Mensaje de OK si pasa el test\n  \"\"\"\n    \n  if type(columns_list) != list:\n    columns_list = [columns_list]\n    \n  schema = T.StructType([T.StructField('metricas', T.StringType(), True)])\n  metricas = spark.createDataFrame(data = [('p0',), ('p25',), ('p40',), ('p50',), ('p60',), ('p75',), ('p100',), ('mean',)], schema = schema).rdd.zipWithIndex().map(lambda row: (row[1], row[0][0]))\n    \n  current_quartiles = spark.createDataFrame(pd.DataFrame(ml_pipe.spark_numeric_vars_quantiles(df, columns_list, quantiles = [0., 0.25, 0.4, 0.5, 0.6, 0.75, 1.0])))\n  \n  columns_list_met = current_quartiles.columns + ['metricas']\n  \n  current_mean = df.select(current_quartiles.columns).describe().where(F.col('summary') == 'mean').drop('summary')\n  current_quartiles = current_quartiles.union(current_mean)\n  current_quartiles = current_quartiles.rdd.zipWithIndex().map(lambda row: (row[1], tuple([row[0][i] for i in range(len(columns_list_met) - 1)])))\n  current_quartiles = current_quartiles.join(metricas).map(lambda row: [row[1][0][i] for i in range(len(columns_list_met) - 1)] + [row[1][1]]).toDF(columns_list_met)\n\n  if distribution_comparation == False:\n    print('Distribucion de las variables numericas:')\n    current_quartiles.show()\n\n  else:\n    previous_quartiles = spark.createDataFrame(pd.DataFrame(ml_pipe.spark_numeric_vars_quantiles(previous_df, columns_list, quantiles = [0., 0.25, 0.4, 0.5, 0.6, 0.75, 1.0])))\n    previous_mean = previous_df.select(current_quartiles.columns).describe().where(F.col('summary') == 'mean').drop('summary')\n    previous_quartiles = previous_quartiles.union(previous_mean)\n    previous_quartiles = previous_quartiles.rdd.zipWithIndex().map(lambda row: (row[1], tuple([row[0][i] for i in range(len(columns_list_met) - 1)])))\n    previous_quartiles = previous_quartiles.join(metricas).map(lambda row: [row[1][0][i] for i in range(len(columns_list_met) - 1)] + [row[1][1]]).toDF(columns_list_met)\n  \n    for col in previous_quartiles.columns:\n      if col != 'metricas':\n        previous_quartiles = previous_quartiles.withColumnRenamed(col, col + '_prev')\n  \n    diff = current_quartiles.join(previous_quartiles, on = 'metricas', how = 'full').na.fill(0)\n\n    for col in diff.columns:\n      if col != 'metricas' and '_prev' not in col:\n        diff = diff.withColumn(col + '_diff', F.udf(lambda x, y: 0 if y == 0 else abs((float(x) - float(y)) / float(y)))(col, col + '_prev'))\n\n    diff = diff.na.fill(0)\n\n    for col in diff.columns:\n      if '_diff' in col:\n        if diff.where(F.col(col) > deviation_threshold).count() > 0:\n          diff.show()\n          raise SystemExit('Hay desviaciones en la distribucion de la variable {}'.format(col))\n        elif diff.where(F.col(col) > 0).count() > 0:\n          print('Diferencias en la distribucion de la variable numerica {}'.format(col))\n          diff.where(F.col(col) > 0).show()\n          print('Hay desviaciones en la distribucion de la columna {0} inferiores al limite {1:.0f}%. En la tabla se muestran algunos casos.'.format(column, deviation_threshold * 100))\n  \n    return 'No hay desviaciones en la distribucion de las variables definidas'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"459319b8-6380-46ae-ab14-d63b5f5ff2eb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def criteria_pointed_check(df, specifications):\n  \n  \"\"\"\n  Test que chequea si la puntuacion de los agentes esta bien calculada con respecto a las condiciones que le corresponderian a cada uno de esos agentes. Hay que darle las especificaciones como una lista siguiendo la estructura de la funcion de filtrado de la libreria datavault dv.func_filtros() de la siguiente forma:\n  \n  - condicion -> tipo: , columna, parametros\n  - puntos ->  tipo: , columna, parametros\n  - calculo_inverso -> Si False (por defecto) indica que se le agregaran puntos al agente por mejorar un indicador. Si True, se le restaran puntos (siniestralidad por ejemplo, a mayor menos puntos para el agente)\n  \n  filtros_sini2 = {'condicion': [{'tipo' : 'rango', 'columna' : 'SINI_NO_VIDA', 'parametros' : [40.01, 70.]}],\n                 'puntos': [{'tipo' : 'rango', 'columna' : 'PTOS_SINI_NO_VIDA', 'parametros' : [6.66, 199.99]}],\n                 'calculo_inverso': True}\n                 \n  filtros_migenerali1 = {'condicion': [{'tipo' : 'rango', 'columna' : 'RATIO_E_CLIENTES', 'parametros' : ['<=', 15.]}],\n                       'puntos': [{'tipo' : 'rango', 'columna' : 'PTOS_E_CLIENTES', 'parametros' : ['=', 0.]}]}\n  \n  La lista specifications debera contener varios elementos como filtros_sini2 arriba: ejemplo: [filtros_migenerali1, filtros_sini2]\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  specifications : list of dicts\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay columnas mal puntuadas, nos indicara en que filas. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  for filtro in specifications:\n    a = filtro['condicion'][0]['parametros']\n    b = filtro['puntos'][0]['parametros']\n    c = filtro['condicion'][0]['columna']\n    d = filtro['puntos'][0]['columna']\n    \n    if 'calculo_inverso' in filtro:\n      calculo_inverso = filtro['calculo_inverso']\n    else:\n      calculo_inverso = None\n\n    if type(a[0]) == str:\n      condiciones = dv.func_filtros(df, filtro['condicion']).count() \n      conteo = dv.func_filtros(df, filtro['puntos']).count() \n      \n      if conteo == condiciones:\n        print('La variable {} esta bien puntuada'.format(c))\n      else:\n        mostrar = dv.func_filtros(df, filtro['condicion']).union(dv.func_filtros(df, filtro['puntos']))\n        mostrar.show()\n        raise SystemExit('La variable {0} tiene {1} valores que cumplen las condiciones {2} y {3} valores con los puntos {4}'.format(c, condiciones, a, conteo, b))\n      \n    else:  \n      if calculo_inverso == True:\n        if b[0] < 0:\n          df = df.withColumn('puntuacion_correcta', F.udf(lambda x, y: 1.0 if abs(((float(x) - (min(a))) * max(b)) - float(y)) <= 1 else 0.0, T.FloatType())(c, d))\n        else:\n          df = df.withColumn('puntuacion_correcta', F.udf(lambda x, y: 1.0 if abs((max(b) - (float(x) - (min(a))) * min(b)) - float(y)) <= 1 else 0.0, T.FloatType())(c, d))\n      else:\n        df = df.withColumn('puntuacion_correcta', F.udf(lambda x, y: 1.0 if abs(((float(x) - (min(a))) * min(b)) - float(y)) <= 1 else 0.0, T.FloatType())(c, d))\n      \n      mostrar = df.where((F.col('puntuacion_correcta') == 0) & (F.col(filtro['condicion'][0]['columna']) <= max(a)) & (F.col(filtro['condicion'][0]['columna']) >= min(a)))\n      if mostrar.count() == 0:\n        print('La variable {} esta bien puntuada'.format(c))\n      else:\n        mostrar.show()\n        raise SystemExit('La variable {0} no esta bien puntuada para los casos mostrados'.format(c))\n    \n  return 'Columnas bien puntuadas OK'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32464d29-9dc4-4608-9140-412750dfca18"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def criteria_pointed_check_planexcelencia(df, specifications):\n  \n  \"\"\"\n  Test que chequea si la puntuacion de los agentes esta bien calculada con respecto a las condiciones que le corresponderian a cada uno de esos agentes. Hay que darle las especificaciones como una lista siguiendo la estructura de la funcion de filtrado de la libreria datavault dv.func_filtros() de la siguiente forma. La diferencia con criteria_pointed_check es que en los rangos intermedios de puntuacion, los incrementos se hacen en 10 puntos basicos, en vez de por punto porcentual:\n  \n  - condicion -> tipo: , columna, parametros\n  - puntos ->  tipo: , columna, parametros\n  - calculo_inverso -> Si False (por defecto) indica que se le agregaran puntos al agente por mejorar un indicador. Si True, se le restaran puntos (siniestralidad por ejemplo, a mayor menos puntos para el agente)\n  \n  filtros_sini2 = {'condicion': [{'tipo' : 'rango', 'columna' : 'SINI_NO_VIDA', 'parametros' : [40.01, 70.]}],\n                 'puntos': [{'tipo' : 'rango', 'columna' : 'PTOS_SINI_NO_VIDA', 'parametros' : [6.66, 199.99]}],\n                 'calculo_inverso': True}\n                 \n  filtros_migenerali1 = {'condicion': [{'tipo' : 'rango', 'columna' : 'RATIO_E_CLIENTES', 'parametros' : ['<=', 15.]}],\n                       'puntos': [{'tipo' : 'rango', 'columna' : 'PTOS_E_CLIENTES', 'parametros' : ['=', 0.]}]}\n  \n  La lista specifications debera contener varios elementos como filtros_sini2 arriba: ejemplo: [filtros_migenerali1, filtros_sini2]\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    DataFrame actual\n  specifications : list of dicts\n  \n  Returns:\n  --------\n  Mensaje de error y la ejecucion se para si hay columnas mal puntuadas, nos indicara en que filas. Mensaje de OK si pasa el test\n  \"\"\"\n  \n  for filtro in specifications:\n    a = filtro['condicion'][0]['parametros']\n    b = filtro['puntos'][0]['parametros']\n    c = filtro['condicion'][0]['columna']\n    d = filtro['puntos'][0]['columna']\n    \n    if 'calculo_inverso' in filtro:\n      calculo_inverso = filtro['calculo_inverso']\n    else:\n      calculo_inverso = None\n\n    if type(a[0]) == str:\n      if b[1] == 0 and c =='OBJ_POL_EXITO_EO':\n        condiciones = df.where((F.col(c) < (a[1])) | (F.col('CLIENTES_VAL') / F.col('OBJ_EX_OPERA_PROP') < 1)).count()\n        conteo = dv.func_filtros(df, filtro['puntos']).count() \n\n        if conteo == condiciones:\n          print('La variable {} esta bien puntuada'.format(c))\n        else:\n          mostrar = dv.func_filtros(df, filtro['condicion']).union(dv.func_filtros(df, filtro['puntos']))\n          mostrar.show()\n          raise SystemExit('La variable {0} tiene {1} valores que cumplen las condiciones {2} y {3} valores con los puntos {4}'.format(c, condiciones , a, conteo, b))\n      elif b[1] == 0:\n        condiciones = df.where((F.col(c) < (a[1]))).count()\n        conteo = dv.func_filtros(df, filtro['puntos']).count() \n\n        if conteo == condiciones:\n          print('La variable {} esta bien puntuada'.format(c))\n        else:\n          mostrar = dv.func_filtros(df, filtro['condicion']).union(dv.func_filtros(df, filtro['puntos']))\n          mostrar.show()\n          raise SystemExit('La variable {0} tiene {1} valores que cumplen las condiciones {2} y {3} valores con los puntos {4}'.format(c, condiciones , a, conteo, b))\n        \n      \n      elif c =='OBJ_POL_EXITO_EO':\n        condiciones = df.where((F.col(c) >= a[1])).where(F.col('CLIENTES_VAL') / F.col('OBJ_EX_OPERA_PROP') >= 1).count()\n        conteo = dv.func_filtros(df, filtro['puntos']).count() \n\n        if conteo == condiciones:\n          print('La variable {} esta bien puntuada'.format(c))\n        else:\n          mostrar = dv.func_filtros(df, filtro['condicion']).union(dv.func_filtros(df, filtro['puntos']))\n          mostrar.show()\n          raise SystemExit('La variable {0} tiene {1} valores que cumplen las condiciones {2} y {3} valores con los puntos {4}'.format(c, condiciones, a, conteo, b))\n      else:\n        condiciones = df.where((F.col(c) >= a[1])).count()\n        conteo = dv.func_filtros(df, filtro['puntos']).count() \n\n        if conteo == condiciones:\n          print('La variable {} esta bien puntuada'.format(c))\n        else:\n          mostrar = dv.func_filtros(df, filtro['condicion']).union(dv.func_filtros(df, filtro['puntos']))\n          mostrar.show()\n          raise SystemExit('La variable {0} tiene {1} valores que cumplen las condiciones {2} y {3} valores con los puntos {4}'.format(c, condiciones, a, conteo, b))\n      \n    else:\n      df = df.withColumn('puntuacion_correcta', F.udf(lambda x, y: 1.0 if abs(((float(x) - (min(a))) / 0.1 * ((max(b) - min(b)) / (max(a) - min (a))) * 0.1) + min(b) - float(y)) <= 1 else 0.0, T.FloatType())(c, d))\\\n      .withColumn('puntuacion_estimada', F.udf(lambda x, y: abs(((float(x) - (min(a))) / 0.1 * ((max(b) - min(b)) / (max(a) - min (a))) * 0.1) + min(b)), T.FloatType())(c, d))\n     \n      \n      mostrar = df.where((F.col('puntuacion_correcta') == 0) & (F.col(filtro['condicion'][0]['columna']) <= max(a)) & (F.col(filtro['condicion'][0]['columna']) >= min(a))\n                        & (F.col('CLIENTES_VAL') / F.col('OBJ_EX_OPERA_PROP') >= 1))\n      \n      if mostrar.count() == 0:\n        print('La variable {} esta bien puntuada'.format(c))\n      else:\n        mostrar.show()\n        raise SystemExit('La variable {0} no esta bien puntuada para los casos mostrados'.format(c))\n    \n  return 'Columnas bien puntuadas OK'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f68153b-b1c8-42aa-8d7d-7b4e5e06f1d9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def sort_schema(sche):\n  \"\"\"\n  Function that sort by field name a given schema\n  \n  Arguments:\n    - sche (StructType): Initial schema.\n    \n  Returns:\n    - sorted schema.\n  \"\"\"\n  \n  \n  sche_dict = {field.name: idx for idx, field in enumerate(sche)}\n  ord_schema = T.StructType([sche[sche_dict[name]] for name in sorted(sche.names)])\n  \n  return ord_schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1e927c0-956b-4e4a-9ca0-3aa523dd20aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def round_numeric_vars(df, exclude_cols = [], scale = 2):\n  \"\"\"\n  Function that rounds the columns of double, float or decimal type to a fixed decimal type for\n  a given dataframe:\n  \n  Arguments:\n    - df (Spark DataFrame): Initial Dataframe to be processed\n    - exclude_cols (list): List that contains the columns to be excluded from the transformation process\n    - scale (int): Indicates the scale to be sused during the rounding procedure\n  \n  Return:\n    - Transformed DataFrame\n  \"\"\"\n  \n  non_decimal_types = [T.StringType(), T.IntegerType(), T.LongType(), T.DateType()]\n  schema = [field for field in df.schema if (field.name not in exclude_cols and field.dataType not in non_decimal_types)]\n  for field in schema:\n    df = df.withColumn(field.name, F.round(field.name, scale).cast(T.DecimalType(18, scale)))\n    \n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c92f182d-d5fa-406f-bd5b-9725821cbc54"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def check_equal_dfs(df1, df2, exclude_conv_cols=[], scale_num_conv=2):\n  \"\"\"\n  Check if two Dataframes are equal. The decimal numerical variables are rounded to\n  DecimalType(18, <scale>) so can be compared neglecting negligible differences. \n  Variables not included in the following types are converted: \n    StringType(), IntegerType(), LongType(), DateType().\n    \n  Args:\n    - df1 (DataFrame): first DataFrame to compare.\n    - df2 (DataFrame): second DataFrame to compare.\n    - exclude_conv_cols (List[String]): list of columns to be excluded from the\n      conversion process.\n    - scale_num_conv (Integer): scale to be used during the conversion process.\n    \n  Returns:\n    - Boolean, indicating if the two Dataframes are equal\n  \"\"\"\n  \n  df1, df2 = df1.na.fill(\"0\"), df2.na.fill(\"0\")\n  df1_count, df2_count = df1.count(), df2.count()\n  schema_df1_s, schema_df2_s = sort_schema(df1.schema), sort_schema(df2.schema)\n  \n  if (df1_count == df2_count) and (schema_df1_s == schema_df2_s):\n  \n    df1_c = round_numeric_vars(df1, exclude_cols=exclude_conv_cols, scale=scale_num_conv).select(schema_df1_s.names)\n    df2_c = round_numeric_vars(df2, exclude_cols=exclude_conv_cols, scale=scale_num_conv).select(schema_df1_s.names)\n    check_count = df1_c.intersect(df2_c).count()\n    \n    return df1_count == check_count\n  \n  else:\n    \n    return False"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32c32ece-14f0-429b-b9f0-deda2b6cbe91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def saving_after_testing_dlake(df, environment, output_table_name, \n                               table_format, partitioned_by=None, \n                               replace_where=None, container=None,\n                               carga ='overwrite'):\n  \n  \"\"\"\n  Funcion que define la ruta de guardado de un proceso dependiendo del tipo de testeo que se este ejecutando\n  \n  Parameters:\n  ------------\n  df : Spark DataFrame\n    Dataframe resultado del proceso\n  environment : String\n    Es el entorno en el que queremos testear el proceso. Puede ser desarrollo para desarrollar, tester para comprobar el codigo antes de subir a produccion, produccion para tests generales de procesos productivos automaticos que guardan en staging. Generalmente vendra dado por los widgets del job\n  output_table_name : String\n    Ruta en la que se guardara la salida del proceso. Generalmente vendra dado por los widgets del job \n   \n  Returns:\n  --------\n  Un dataframe que se guardara en la ruta dada, devolvera mensaje SUCCESS. Si el entorno se introduce erroneamente devolvera un mensaje de error\n  \"\"\"\n  \n  testing_path = 'testing/desarrollo/'\n  testing_path_tester = 'testing/tester/'\n  contianer_dev_test = \"usuarios\"\n  table_format_dev_test = \"parquet\"\n  \n  if environment in ['desarrollo', 'dev']:\n    \n    gen_utils.write_dlake_table(df=df, \n                                table_path=os.path.join(testing_path, output_table_name), \n                                container=contianer_dev_test, \n                                mode=carga, \n                                environment=\"desarrollo\", \n                                table_format=table_format_dev_test, \n                                partitioned_by=partitioned_by)\n    \n    return output_table_name\n    \n  elif environment == 'tester':\n    \n    gen_utils.write_dlake_table(df=df, \n                            table_path=os.path.join(testing_path_tester, output_table_name), \n                            container=contianer_dev_test, \n                            mode=carga, \n                            environment=\"desarrollo\", \n                            table_format=table_format_dev_test, \n                            partitioned_by=partitioned_by, \n                            replace_where=replace_where)\n    \n    return output_table_name\n  \n  elif environment in ['produccion', 'pro']:\n    \n    gen_utils.write_dlake_table(df=df, \n                            table_path=output_table_name, \n                            container=container, \n                            mode=carga, \n                            environment=environment, \n                            table_format=table_format, \n                            partitioned_by=partitioned_by, \n                            replace_where=replace_where)\n    \n    return \"SUCCESS\"\n    \n    \n  elif environment in ['pre-produccion', 'pre-pro']:\n    \n    gen_utils.write_dlake_table(df=df, \n                            table_path=output_table_name, \n                            container=container, \n                            mode=carga, \n                            environment='dev', \n                            table_format=table_format, \n                            partitioned_by=partitioned_by, \n                            replace_where=replace_where)\n    \n    return \"SUCCESS\"\n    \n\n  else:\n    raise SystemExit('El entorno debe ser desarrollo, tester o produccion')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bc05475-e27e-49ce-8238-b0e5c1735582"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"funciones_testing","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2923418674705782}},"nbformat":4,"nbformat_minor":0}
