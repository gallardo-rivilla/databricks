{"cells":[{"cell_type":"markdown","source":["### FUNCIONES GENERALES\n\n###### **Data reading**\n- ``read_path_data(data_file, dataframe_format, sep = ',', inferSchema = True, header = True, encoding = 'utf-8', schema = None)``: returns a Spark or pandas dataframe so the data scientist can work with it on Databricks not needing to transform the data manually regardless of its format\n- ``read_dwh_table(query, entorno=\"pro\", column=None, lower_bound=None, upper_bound=None, num_partitions=50)``: read tables from the Data WareHouse with an SQL query and lower the column names\n- ``read_staging_table(table, entorno=\"pro\", schema=\"VARCALC\", lower_col_names=True, column=None, lower_bound=None, upper_bound=None, num_partitions=50)``: read table from STAGING.\n- ``write_staging_table(df, table, mode)``: write dataframe(df) to STAGING table\n- ``write_staging_table_truncate(df, table, mode)``: write dataframe(df) to STAGING table keeping the schema defined in DWH SQL\n- ``read_dlake_table(table_path, container, environment=\"dev\", table_format=\"delta\", header=True, sep=\",\")``: function that reads a table from the Data Lake.\n- ``write_dlake_table(df, table_path, container, mode = \"overwrite\", environment = \"dev\", table_format = \"delta\", partitioned_by = None, replace_where = None, header = True, sep = \",\")``: function to save a table in the DataLake.\n\n###### Spark dataframes utils\n- ``spark_dataframe_shape(df)``: returns a list with the shape of a spark dataframe including rows and columns in the first and second element correpondingly.\n- ``spark_lower_column_names(df)``: lower the column names\n- ``spark_upper_column_names(df)``: upper the column names\n- ``spark_numeric_variables(df)``: returns a list with column names of those columns with any numeric spark sql type.\n- ``spark_string_variables(df)``: returns a list with column names of those columns with a StringType spark sql type\n- ``spark_datetime_variables(df)``: returns a list with column names of those columns with a Datetime spark sql type\n- ``spark_datetime_var_split(df, datetime_vars)``: If a spark DataFrame includes any datetime type variable, it will split them in 3 columns of day, month and year so they can be analyzed separatelly and deletes the date variables. The new columns will be named the same as the original one including \"_day\", \"_month\" or \"_year\"\n- ``convert_numeric_variables_to_float(df):``: convierte las variables numericas de un dataframe a formato float\n\n###### General utils\n- ``spark_written_csv_rename(csv_path, desired_name)``: as Hadoop generates strange names when creating a csv file and some control files that will fill up our Blob Storage with rubbish, this function renames that file to a desired name and deletes all useless stuff\n- ``delete_blob_folder(path)``: deletes all files in a specified path\n- ``get_lastday_lastmonth(fecha)``: calculates the last day of the last month to the given date in YYYYMMDD format\n- ``incrementos_periodo(fecha_inicial, unidad_temporal, incremento, formato_salida, cierre)``: Adds or subtracts a given time interval to an initial date in YYYYMMDD format.\n- ``unpersist_rdd(table_id=None, unpersist_all=False)``: Function to unpersist cached RDDs from memory or disk.\n- ``calculate_business_days(input_date)``: Calculates the volume of business days from a given date.\n- ``get_previous_day(fecha)``: Given a date in yyyyMMdd format, returns the previous day.\n- ``get_today()``: Returns todays's date in yyyyMMdd format.\n- ``get_yesterday()``: Returns yesterday's date in yyyyMMdd format.\n- `spark_dim_window_filter(tabla, fecha_hub, claves, drop = True, registro_fecha = \"fecha_inicio\")`: Filtra registros de un periodo en la tabla de dimensiones sin usar claves subrogadas."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61d258a2-ea0a-4e12-a321-510b99594867"}}},{"cell_type":"code","source":["import os\nimport pandas as pd\nimport numpy as np\nimport shutil\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pandas.tseries.offsets import BDay\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nfrom adal import AuthenticationContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd48b695-3b00-48d1-8db5-edb2db565ae6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Data sources connect information"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22cc634b-5d48-41ce-86ae-93176793c5c0"}}},{"cell_type":"code","source":["# BlobStorage connecting information\nstorage_account_name = \"3satoristgaccountmodelos\"\nstorage_account_access_key = \"FZDvphbs+NXwzQpuDUz+8ZQToxxgDX5Dab1gpG1ZGgWENq7qgR33FqL3MpsMbUc29Tzf9HJQd/hPPzgwvhplcw==\"\n\nspark.conf.set(\n  \"fs.azure.account.key.\" + storage_account_name + \".blob.core.windows.net\", \n  storage_account_access_key)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55d36d18-fa47-42e4-950f-bc18397af587"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Data reading"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cab07d6a-4105-4558-8e1c-f7929e7a92af"}}},{"cell_type":"code","source":["def _get_token(entorno=\"pro\"):\n  \"\"\"\n  Get token necessary for Service Principal autentication.\n  PRIVATE USAGE ONLY!\n  \n  Parameters:\n  ------------\n  entorno : string\n    Indicates the SQL server environment. Valid options:\n        - \"pro\" -> production (3)\n        - \"dev\" -> development (6)\n\n  Returns:\n  --------\n  Tuple with 3 itmes:\n    - jdbc_url_sp : string containing the url of de DWH database\n    - jdbc_url_staging_sp : string containing the url of de Staging database\n    - properties_sp : diccionary contaning the configuration of the Service Principal\n    \n  \"\"\"\n  \n  if entorno not in [\"pro\", \"dev\"]:\n    raise ValueError(\"Parameter 'entorno' must be either 'pro' or 'dev'.\")\n  \n  entorno_code = {\"pro\": 3, \"dev\": 6}.get(entorno)\n  \n  authority_url = \"https://login.microsoftonline.com/cbeb3ecc-6f45-4183-b5a8-088140deae5d\"\n  resource = \"https://database.windows.net/\"\n  context = AuthenticationContext(authority_url)\n  client_id = \"4064fe44-fe06-4978-8e43-2a8fcf9d99cd\"\n  token_sp = (context\n             .acquire_token_with_client_credentials(resource, \n                                                    client_id, \n                                                    dbutils.secrets.get(scope=\"key_prod\", key=\"sp_prod\")))\n  jdbc_server_sp = '{0}-satori-dwh-server.database.windows.net'.format(entorno_code)\n  jdbc_database_sp = 'SOVSAT{0}'.format(entorno_code)\n  jdbc_database_staging_sp = 'STAGING'\n  jdbc_url_sp = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbc_server_sp, 1433, jdbc_database_sp) \n  jdbc_url_staging_sp = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbc_server_sp, 1433, jdbc_database_staging_sp) \n  properties_sp = {\n      \"accessToken\" : token_sp[\"accessToken\"], \n      \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n      \"hostNameInCertificate\" : \"*.database.windows.net\", \n      \"encrypt\": \"true\"\n  }\n  \n  return jdbc_url_sp, jdbc_url_staging_sp, properties_sp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eee87697-09ec-4e50-a0cb-87a76d5e091c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def read_staging_table(table, entorno=\"pro\", schema=\"VARCALC\", lower_col_names=True,\n                       column=None, lower_bound=None, upper_bound=None, num_partitions=50):\n  \n  \"\"\"\n  Read a table from the Staging database \n  \n  Parameters:\n  ------------\n  - table : string\n    Name table to send to the jdbc.\n    \n  - entorno : string\n    Indicates the SQL server environment. Valid options:\n        - \"pro\" / \"produccion\" -> production (3)\n        - \"dev\" / \"desarrollo\" -> development (6)\n        \n  - schema : string\n    Schema of the Staging database to be used\n    \n  - lower_col_names : Boolean\n    Boolean that indicates if the column names of the table should be lowered\n    \n    \n  If the user wants to parallelize the reading process, the following parameters should be specified:\n  \n  - column : string\n    Reference column to be used during the partitioning process. It should be an Integer, Date or TimeStamp Type\n    \n  - lower_bound : Integer / Date / TimeStamp\n    Smallest value of the reference column.\n    \n  - upper_bound : Integer / Date / TimeStamp\n    Biggest value of the reference column.\n\n  - num_partitions : Integer.\n    Number of partitions to be used.\n        \n  \n  Returns:\n  --------\n  A dataframe with the data received from the DWH after applying the query.\n  \"\"\"  \n  \n  table = \"{0}.{1}\".format(schema, table)\n  entorno_code = {\"pro\": \"pro\", \"dev\": \"dev\", \"produccion\": \"pro\", \"desarrollo\": \"dev\"}[entorno]\n  jdbc_url_sp, jdbc_url_staging_sp, properties_sp = _get_token(entorno_code)\n  \n  if (column is None) and (lower_bound is None) and (upper_bound is None):\n  \n    df = spark.read.jdbc(jdbc_url_staging_sp, table, properties=properties_sp)\n    \n  else:\n  \n      df = spark.read.jdbc(url=jdbc_url_staging_sp, \n                           table=table, \n                           properties=properties_sp, \n                           column=column, \n                           lowerBound=lower_bound, \n                           upperBound=upper_bound, \n                           numPartitions=num_partitions)\n    \n  if lower_col_names:\n    df = spark_lower_column_names(df)\n  \n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e69ea683-41c5-473a-8d8a-39cc7275b543"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_path_data(data_file, dataframe_format, sep = ',', inferSchema = True, header = True, encoding = 'utf-8', schema = None):\n  \n  \"\"\"\n  Returns a Spark or pandas dataframe so the data scientist can work with it on Databricks not needing to transform the data manually regardless of its format.\n  \n  Parameters:\n  ------------\n  data_file : string\n    Path to the input data\n  dataframe_format : string\n    Format of the output dataframe, it can be either spark or pandas\n  sep : string\n    Columns separator character, ',' by default\n  inferSchema : boolean\n    Only on spark dataframes. Whether the schema is infered or not. True by default\n  header : boolean\n    Whether the data includes a header or not. True by default\n  encoding : string\n    Data encoding. utf-8 by default\n  \n  Returns:\n  --------\n  A dataframe of dataframe_format.\n  \"\"\"\n  \n  file_format = data_file.split('.')[-1]\n  \n  if dataframe_format == 'spark':\n  \n    if file_format == 'csv':\n      if schema!=None:\n        df = spark.read.format(file_format).load(data_file, schema=schema, delimiter=sep, header = header, encoding = encoding)\n      else:\n        df = spark.read.format(file_format).load(data_file, inferSchema = inferSchema, delimiter=sep, header = header, encoding = encoding)\n        \n    elif file_format == 'sas7bdat':\n      sas_table = pd.read_sas(data_file.replace('dbfs:/', '/dbfs/'), encoding = encoding)\n      if schema != None:\n        df = spark.createDataFrame(sas_table, schema = schema)\n      else:\n        df = spark.createDataFrame(sas_table)\n        \n    for column in df.columns:\n      df = df.withColumnRenamed(column,column.lower())\n      df = df.withColumnRenamed(column,column.replace(' ','_').replace('.','_').replace(';','_').replace(',','_').replace('-','_').replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u'))\n      \n  return df\n\n#-------------------------------------\n\ndef read_dwh_table(query, entorno=\"pro\", \n                   column=None, lower_bound=None, upper_bound=None, num_partitions=50):\n  \n  \"\"\"\n  Read tables from the Data WareHouse with an SQL query and lower the column names.\n  \n  Parameters:\n  ------------\n  - query : string\n    An SQL query to send to the jdbc. Must contain the columns selected and the DWH database.table.\n    \n  - entorno : string\n    Indicates the SQL server environment. Valid options:\n        - \"pro\" / \"produccion\" -> production (3)\n        - \"dev\" / \"desarrollo\" -> development (6)\n        \n  If the user wants to parallelize the reading process, the following parameters should be specified:\n  \n  - column : string\n    Reference column to be used during the partitioning process. It should be an Integer, Date or TimeStamp Type\n    \n  - lower_bound : Integer / Date / TimeStamp\n    Smallest value of the reference column.\n    \n  - upper_bound : Integer / Date / TimeStamp\n    Biggest value of the reference column.\n\n  - num_partitions : Integer.\n    Number of partitions to be used.\n    \n  \n  Returns:\n  --------\n  A dataframe with the data received from the DWH after applying the query.\n  \"\"\"\n  \n  entorno_code = {\"pro\": \"pro\", \"dev\": \"dev\", \"produccion\": \"pro\", \"desarrollo\": \"dev\"}[entorno]\n  jdbc_url_sp, jdbc_url_staging_sp, properties_sp = _get_token(entorno_code)\n  \n  if (column is None) and (lower_bound is None) and (upper_bound is None):\n    \n      df = spark.read.jdbc(url=jdbc_url_sp, \n                       table=query, \n                       properties=properties_sp)\n    \n    \n  else:\n  \n    df = spark.read.jdbc(url=jdbc_url_sp, \n                         table=query, \n                         properties=properties_sp, \n                         column=column, \n                         lowerBound=lower_bound, \n                         upperBound=upper_bound, \n                         numPartitions=num_partitions)\n  \n  df = spark_lower_column_names(df)\n  \n  return df  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e23320d-5bd6-46ff-88d2-fcaa5a509103"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_staging_table(df, table, mode, entorno=\"pro\", schema=\"VARCALC\"):\n  \n  \"\"\"\n  Write df to Staging database\n  \n  Parameters:\n  ------------\n  df: dataframe to save\n  \n  table : staging name table\n  \n  mode: 'Overwrite'/ 'Append'\n  \n  entorno : string\n    Indicates the SQL server environment. Valid options:\n        - \"pro\" / \"produccion\" -> production (3)\n        - \"dev\" / \"desarrollo\" -> development (6)\n\n  schema : string\n    Schema of the Staging database to be used\n  \n  Returns:\n  --------\n  Nothing, this function just runs a process\n  \"\"\"  \n  entorno_code = {\"pro\": \"pro\", \"dev\": \"dev\", \"produccion\": \"pro\", \"desarrollo\": \"dev\"}[entorno]\n  table = \"{0}.{1}\".format(schema, table)\n  jdbc_url_sp, jdbc_url_staging_sp, properties_sp = _get_token(entorno_code)\n  df.write.mode(mode).jdbc(jdbc_url_staging_sp, table, properties=properties_sp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b6dc56b-cfc2-4cdc-989d-dfe5aadc944a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def write_staging_table_truncate(df, table, mode, entorno=\"pro\", schema=\"VARCALC\"):\n  \n  \"\"\"\n  Write df to Staging database (truncate option enabled)\n  \n  Parameters:\n  ------------\n  df: dataframe to save\n  \n  table : staging name table\n  \n  mode: 'Overwrite'/ 'Append'\n  \n  entorno : string\n    Indicates the SQL server environment. Valid options:\n        - \"pro\" / \"produccion\" -> production (3)\n        - \"dev\" / \"desarrollo\" -> development (6)\n\n  schema : string\n    Schema of the Staging database to be used\n  \n  Returns:\n  --------\n  Nothing, this function just runs a process\n  \"\"\"  \n  entorno_code = {\"pro\": \"pro\", \"dev\": \"dev\", \"produccion\": \"pro\", \"desarrollo\": \"dev\"}[entorno]\n  table = \"{0}.{1}\".format(schema, table)\n  jdbc_url_sp, jdbc_url_staging_sp, properties_sp = _get_token(entorno_code)\n  (df.write\n   .mode(mode)\n   .option(\"truncate\",True)\n   .jdbc(jdbc_url_staging_sp, table, properties=properties_sp))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3ec2a2f-8620-4fe2-99d4-a31da0b23175"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Spark dataframes utils"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1917865c-bd09-473f-9f3c-184c03b0edd4"}}},{"cell_type":"code","source":["def spark_dataframe_shape(df):\n  \n  \"\"\"\n  Returns a list with the shape of a spark dataframe including rows and columns in the first and second element correpondingly.\n  \n  Parameters:\n  ------------\n  df : spark DataFrame\n    The dataframe of which we want to calculate the shape\n  \n  Returns:\n  --------\n  A list with rows on the first element and columns on the second.\n  \"\"\"\n  print('The dataframe has {} rows and {} columns'.format(df.count(), len(df.columns)))\n  return [df.count(), len(df.columns)]\n\n#-------------------------------------\n\ndef spark_lower_column_names(df):\n  \n  \"\"\"\n  Lower the column names.\n  \n  Parameters:\n  ------------\n  df : spark DataFrame\n    The dataframe of which we want to lower the column names\n  \n  Returns:\n  --------\n  The same dataframe with the same column names in lower.\n  \"\"\"\n  \n  for column in df.columns:\n      \n      df = df.withColumnRenamed(column,column.lower())\n  \n  return df\n\n#-------------------------------------\n\ndef spark_upper_column_names(df):\n  \n  \"\"\"\n  Upper the column names.\n  \n  Parameters:\n  ------------\n  df : spark DataFrame\n    The dataframe of which we want to upper the column names\n  \n  Returns:\n  --------\n  The same dataframe with the same column names in upper.\n  \"\"\"\n  \n  for column in df.columns:\n      \n      df = df.withColumnRenamed(column,column.upper())\n  \n  return df\n\n#-------------------------------------\n\ndef spark_numeric_variables(df):\n  \n  \"\"\"\n  Returns a list with column names of those columns with any numeric spark sql type.\n  \n  Parameters:\n  ------------\n  df : spark DataFrame\n    The dataframe that we want to analyze\n  \n  Returns:\n  --------\n  A list with the names of those columns that are numeric.\n  \"\"\"\n  \n  # create a list with all variables which dataType is numeric\n  list_column_names = [variable.name for variable in df.schema.fields if ('IntegerType' in str(variable.dataType)\n                                                                or 'DoubleType' in str(variable.dataType) \n                                                                or 'DecimalType' in str(variable.dataType)\n                                                                or 'FloatType' in str(variable.dataType)\n                                                                or 'LongType' in str(variable.dataType)\n                                                                or 'ShortType' in str(variable.dataType)\n                                                                or 'ByteType' in str(variable.dataType))]\n  \n  return list_column_names\n\n#-------------------------------------\n\ndef spark_string_variables(df):\n  \n  \"\"\"\n  Returns a list with column names of those columns with a StringType spark sql type.\n  \n  Parameters:\n  ------------\n  df : spark DataFrame\n    The dataframe that we want to analyze\n  \n  Returns:\n  --------\n  A list with the names of those columns that are StringType.\n  \"\"\"\n  \n  # create a list with all variables which dataType is numeric\n  list_column_names = [variable.name for variable in df.schema.fields if variable.dataType == T.StringType()]\n  \n  return list_column_names\n\n#-------------------------------------\n\ndef spark_datetime_variables(df):\n  \n  \"\"\"\n  Returns a list with column names of those columns with a Datetime spark sql type.\n  \n  Parameters:\n  ------------\n  df : spark DataFrame\n    The dataframe that we want to analyze\n  \n  Returns:\n  --------\n  A list with the names of those columns that are StringType.\n  \"\"\"\n  \n  # create a list with all variables which dataType is numeric\n  list_column_names = [variable.name for variable in df.schema.fields if variable.dataType in \n                       (TimestampType(), DateType())]\n  \n  return list_column_names\n\n#-------------------------------------\n\ndef spark_datetime_var_split(df, datetime_vars):\n  \n  \"\"\"\n  If a spark DataFrame includes any datetime type variable, it will split them in 3 columns of day, month and year so they can be analyzed separatelly and deletes the date variables. The new columns will be named the same as the original one including \"_day\", \"_month\" or \"_year\".\n  \n  Parameters:\n  ------------\n  df : spark DataFrame\n    The dataframe that we want to analyze\n  datetime_vars: list\n    A list including all Spark Date type variables\n  \n  Returns:\n  --------\n  A dataframe with 3 new columns for each date type variable including its day, month and year and deleting the original variable.\n  \"\"\"\n    \n  for i in range(len(datetime_vars)):\n    df = df.withColumn(datetime_vars[i] + '_day', F.dayofmonth(F.col(datetime_vars[i])))\n    df = df.withColumn(datetime_vars[i] + '_month', F.month(F.col(datetime_vars[i])))\n    df = df.withColumn(datetime_vars[i] + '_year', F.year(F.col(datetime_vars[i])))\n    df = df.drop(datetime_vars[i])\n    \n  return df\n\n#-------------------------------------\n\ndef columns_name_normalization(df, replacing_character = '_'):\n  \n  \"\"\"\n  Renames all columns of a dataframe in order to replace non allowed characters.\n  \n  Parameters:\n  ------------\n  df : spark DataFrame\n  \n  replacing_character : String\n    The character by which we would like to replace the non allowed ones. _ by default\n  \n  Returns:\n  --------\n  A dataframe with 3 new columns for each date type variable including its day, month and year and deleting the original variable.\n  \"\"\"\n\n  for column in df.columns:\n    df = df.withColumnRenamed(column, column.replace(' ', replacing_character).replace('.', replacing_character).replace(';', replacing_character).replace(',', replacing_character).replace('-', replacing_character))\n    \n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4687302-cd76-477a-9679-bf981433b633"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["General utils"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77191485-f234-4f09-94c4-ee3c90e8ff2f"}}},{"cell_type":"code","source":["def spark_written_csv_rename(csv_path, desired_name):\n  \n  \"\"\"\n  As Hadoop generates strange names when creating a csv file and some control files that will fill up our Blob Storage with rubbish, this function renames that file to a desired name and deletes all useless stuff.\n  \n  Parameters:\n  -----------\n  csv_path: Path in which the csv file is saved by spark\n    string\n  desired_name: Name we would like the csv file to have. The .csv format is not needed as it will write it automatically.\n    string\n  \n  Returns:\n  --------\n  Nothing, this function just runs a process\n  \"\"\"\n  \n  os_path = csv_path.replace('dbfs:','/dbfs')\n  list_files = []\n  \n  for (os_path, folders, files) in os.walk(os_path):\n    list_files.extend(files)\n    break\n  \n  for file in list_files:\n       \n    if file.split(\".\")[-1] == 'csv' and file[:10] == 'part-00000':\n      os.rename(os_path + '/' + file, os_path + '/' + desired_name + '.csv')\n      \n    else: \n      os.remove(os_path + file)\n\n#-------------------------------------\n\ndef delete_blob_folder(path):\n  \n  \"\"\"\n  deletes all files in a specified path  \n  \n  Parameters:\n  -----------\n  path: Path with the folders in the blob we want to detele\n    string\n  \n  Returns:\n  --------\n  Prints the folders / files that are being deleted under the given path\n  \"\"\"\n  \n  os_path = path.replace('dbfs:','/dbfs')\n  list_folders = []\n  list_files = []\n  \n  for (os_path, folders, files) in os.walk(os_path):\n    list_folders.extend(folders)\n    break\n  \n  for folder in list_folders:  \n    shutil.rmtree(os_path + folder)\n    print(folder + ' removed')\n    \n  for (os_path, folders, files) in os.walk(os_path):\n    list_files.extend(files)\n    break\n    \n  for file in list_files:\n    os.remove(os_path + file)\n    print(file + ' removed')\n\n#-------------------------------------\n    \n  \n# Refactorizada en la siguiente celda usando las funciones incrementos_periodo() y is_last_day_month()\n\n# def get_lastday_lastmonth(fecha):\n  \n#   \"\"\"\n#   Calculates the last day of the last month to the given date in YYYYMMDD format \n  \n#   Parameters:\n#   -----------\n#   fecha: Date in YYYYMMDD format \n#     string\n  \n#   Returns:\n#   --------\n#   last day of the last month to the given date in YYYYMMDD format\n#   \"\"\"\n    \n#   fecha_sep = datetime.datetime(int(fecha[:4]), int(fecha[4:6]), int(fecha[6:]))\n#   first_day = fecha_sep.replace(day=1)\n#   prev_month_lastday = first_day - datetime.timedelta(days=1)\n#   ultimo_dia_ultimo_mes = str(prev_month_lastday.year) + str(prev_month_lastday.month).zfill(2) + str(prev_month_lastday.day).zfill(2)\n  \n#   today_plus_one = fecha_sep + datetime.timedelta(days=1)\n#   if today_plus_one.month != fecha_sep.month:\n#     ultimo_dia_ultimo_mes = fecha\n  \n#   return ultimo_dia_ultimo_mes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73cf38e6-b198-4b94-ae21-bf890f514a4a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def incrementos_periodo(periodo_ini, unidad_temporal='dia', incremento=0, formato_salida='completo', cierre=None): \n  '''\n    Función que suma o resta periodos temporales a la fecha inicial introducida. \n    Por defecto devuelve el día de ayer. \n    \n    Parámetros: \n    ------------\n    periodo_ini (str): \n    Fecha de la que se parte para los calculos. \n    Si es un int pasarlo como str(int). Formato 'YYYYMMDD'. Ej: '20191031'\n    \n    unidad_temporal (str): \n    Unidad temporal que se quiere incrementar. \n    Valores admitidos: 'dia', 'mes', 'ano'.\n    \n    incremento (int): \n    Cantidad de unidades temporales a incrementar: positivo suma, negativo resta\n    \n    formato_salida (str): \n    Formato de fecha deseado en la salida. \n    Valores admitidos: 'dia', 'mes', 'ano', 'completo', 'periodo', 'datetime'.\n  \n    cierre (str): None por defecto. \n    Develve el cierre mensual o anual del periodo calculado. \n    Valores admitidos: 'mensual', 'anual'\n\n    Returns:\n    -----------\n    fecha_salida (str): \n    Fecha de salida con el formato especificado (formato_salida). \n    Si el formato de salida es 'datetime' el tipo de datos que devuelve es datetime.datetime\n\n    Ejemplos de uso: \n    ------------------\n    Calcular el cierre mes actual, formato salida 'completa': \n    >>> incrementos_periodo('20191226', incremento = 0, cierre = 'mensual')\n    >>> '20191231'\n    \n    Calcular el cierre anual anterior, formato salida 'datetime': \n    >>> incrementos_periodo('20191226', unidad_temporal = ano, incremento = -1, formato_salida = 'datetime', cierre = 'anual')\n    >>> datetime.datetime(2018, 12, 31, 0, 0)\n\n    Calcular el dia restando 15 dias, salida formato salida 'dia':\n    >>> incrementos_periodo('20191226', unidad_temporal = 'dia', incremento = -15, formato_salida = 'dia')\n    >>> '11'\n\n    Calcular periodo del mes anterior, formato salida 'periodo': \n    >>> incrementos_periodo('20191226', unidad_temporal = 'mes', incremento = -1, formato_salida = 'periodo')\n    >>> '201911'\n  '''\n  \n  # Comprobar valores de los parametros introducidos\n  if len(periodo_ini) != 8:\n    print('El formato de fecha introducido no es válido. Recuerda que tiene que tener formato YYYYMMDD, con 0 si el mes o el día son < 10 → Ej.: 20191105')\n    return\n    \n  if unidad_temporal not in ['dia', 'mes', 'ano']:\n    print('La unidad temporal introducida no es válida. Los valores aceptados son: \\'dia\\', \\'mes\\' o \\'ano\\'')\n    return    \n  \n  if formato_salida not in ['dia', 'mes', 'ano', 'completo', 'periodo', 'datetime']:\n    print('El formato de fecha para la salida no es válido. Los valores aceptados son: \\'dia\\', \\'mes\\' o \\'ano\\', \\'completo\\', \\'periodo\\' o \\'datetime\\'')\n    return    \n  \n  # Convertir fecha inicial periodo_ini a formato datetime\n  try: \n    datetime_object = datetime.datetime.strptime(periodo_ini, '%Y%m%d')\n  except:\n    print('El formato de fecha introducido no es válido.')\n    return\n\n  # Incrementar segun unidad_temporal e incremento\n  new_datetime = datetime_object\n  incrementos_dict = {\n                        'dia':datetime.timedelta(days=incremento), \n                        'mes':relativedelta(months=incremento), \n                        'ano':relativedelta(years=incremento)\n                    }\n\n  new_datetime += incrementos_dict[unidad_temporal]\n  \n  # Si queremos el cierre mensual o anual del periodo calculado\n  if cierre == 'anual':\n    # Reemplazar y sumar dias para pasar al mes siguiente\n    new_datetime = new_datetime.replace(month=1) + relativedelta(months=+12)\n    # Ultimo dia del mes anterior\n    new_datetime = new_datetime - relativedelta(months=new_datetime.month)\n\n  if (cierre == 'mensual') | (cierre == 'anual'): \n    # Reemplazar y sumar dias para pasar al mes siguiente\n    new_datetime = new_datetime.replace(day=1) + datetime.timedelta(days=31)\n    # Ultimo dia del mes anterior\n    new_datetime = new_datetime - datetime.timedelta(days=new_datetime.day)\n  \n  # Devolver fecha completa, dia, mes, ano, periodo o datetime segun especificado\n  dict_salidas = {\n                    'completo':new_datetime.strftime('%Y%m%d'), \n                    'dia':str(new_datetime.day), \n                    'mes':str(new_datetime.month), \n                    'ano':str(new_datetime.year), \n                    'periodo':new_datetime.strftime('%Y%m'), \n                    'datetime': new_datetime\n                }\n\n  fecha_salida =  dict_salidas[formato_salida]\n\n  return fecha_salida\n\ndef is_last_day_month(fecha):\n  \"\"\"\n  Checks if date is last day of the current month\n  \n  Parameters:\n  -----------\n  fecha: Date in YYYYMMDD format \n    string\n    \n  Returns:\n  --------\n  (boolean)\n  \"\"\"\n\n  lastday_thismonth = incrementos_periodo(fecha, 'mes', 0, 'completo', 'mensual')\n  return (lastday_thismonth == fecha)\n\ndef get_lastday_lastmonth(fecha):\n  \n  \"\"\"\n  Calculates the last day of the last month to the given date in YYYYMMDD format.\n  If given date is last day of month, it returns raw given date. \n  \n  Parameters:\n  -----------\n  fecha: Date in YYYYMMDD format \n    string\n  \n  Returns:\n  --------\n  last day of the last month to the given date in YYYYMMDD format\n  \"\"\"\n  \n  lastday_lastmonth = incrementos_periodo(fecha, 'mes', -1, 'completo', 'mensual')\n  return fecha if (is_last_day_month(fecha)) else lastday_lastmonth"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f620ff0-a810-46ee-955d-3e25c97fca5f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def unpersist_rdd(table_id=None, unpersist_all=False):\n  \"\"\" \n  Function to unpersist cached DataFrames from memory and disk.\n  This function is meant to be used when you no longer have any reference to the DataFrame/RDD to unpersist. Otherwise you can use df.unpersist() or rdd.unpersist()\n  \n  Parameters:\n    table_id (int): \n    ID of the RDD to unpersist, it can be obtained from the Storage tab on Spark UI.\n    \n    unpersist_all (Boolean, False by default):\n    If true, unpersists all DataFrames on memory..      \n  \n  Returns: \n    Nada, simplemente borra el(los) RDD(s) seleccionado(s). \n  \"\"\"\n  \n  if unpersist_all == True: \n    for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n      rdd.unpersist()\n      print(\"Unpersisted {} rdd\".format(id))\n      \n  else:    \n    try: \n      spark._jsc.getPersistentRDDs()[table_id].unpersist()\n\n    except AttributeError as e:\n      print(\"RDD with {} id not found\".format(table_id))\n      if isinstance(table_id, int) == False:\n        print(\"table_id must be an INTEGER\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcb434e1-7276-4c56-a883-30f64b73f6e2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def convert_numeric_variables_to_float(df):\n  \"\"\" \n  Convierte las variables numericas de un dataframe a formato float\n  \n  Parameters:\n  df (Spark DataFrame)\n\n  Returns: \n    El mismo dataframe con las columnas numericas cambiadas a float \n  \"\"\"\n\n  # create a list with all variables which dataType is numeric\n  list_column_names = [variable.name for variable in df.schema.fields if ('IntegerType' in str(variable.dataType)\n                                                                or 'DoubleType' in str(variable.dataType) \n                                                                or 'DecimalType' in str(variable.dataType)\n                                                                or 'FloatType' in str(variable.dataType)\n                                                                or 'LongType' in str(variable.dataType)\n                                                                or 'ShortType' in str(variable.dataType)\n                                                                or 'ByteType' in str(variable.dataType))]\n  for col_name in list_column_names:\n    df = df.withColumn(col_name, F.col(col_name).cast('float'))\n            \n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"167774c4-68ce-402d-bff4-cb051b48a97e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def calculate_business_days(input_date):\n  \"\"\"\n  Calculates the volume of business days from a given date\n  \n  Parameters:\n  -----------\n  input_date: Date in YYYYMMDD or datetime format \n    string/date\n    \n  Returns:\n  --------\n  integer with number of business days\n  \"\"\"\n  \n  if type(input_date) != datetime.date:\n    datetime_date = datetime.datetime.strptime(input_date, '%Y%m%d')\n  else:\n    datetime_date = input_date\n    \n  for i in range(datetime_date.month * 11):\n    if datetime_date.month == 1:\n      estimated_Bdays = datetime_date.day - i\n    else:\n      estimated_Bdays = (datetime_date.month - 1) * 22 + datetime_date.day - i\n\n    estimated_end_year = datetime_date - BDay(estimated_Bdays)\n    if estimated_end_year.day == 31 and estimated_end_year.month == 12:\n      actual_Bdays = estimated_Bdays\n      \n  return actual_Bdays"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39a48cf8-0a9a-4f2e-9e2b-2563253f53b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_dlake_table(table_path, container, environment=\"dev\", table_format=\"delta\", header=True, sep=\",\"):\n  \"\"\"\n  Function that reads a table from the Data Lake.\n  \n  Args:\n    - table_path (String): Path where the table is saved. Can't start with '/'.\n    - container (String): Container of the Data Lake where the table is stored.\n    - environment (String): 'dev' / 'desarrollo' or 'pro' / 'produccion'.\n    - table_format (String): Format of the table (parquet, PARQUET, delta, DELTA, csv, CSV).\n    - header (Boolean): Boolean that indicates the header option (only for CSV tables).\n    - sep (String): Separator (only for CSV tables).\n    \n  Returns:\n    - Spark DataFrame.\n  \"\"\"\n  \n  environment_code = {\"pro\": \"pro\", \"dev\": \"dev\", \"produccion\": \"pro\", \"desarrollo\": \"dev\"}[environment]\n  # Check functions inputs\n  if not environment in [\"dev\", \"pro\", \"desarrollo\", \"produccion\"]:\n    raise ValueError(\"'environment' must be one of the following options: 'dev' or 'pro'. {0} found.\".format(environment))\n  \n  if not table_format in [\"parquet\", \"PARQUET\", \"delta\", \"DELTA\", \"csv\", \"CSV\"]:\n    raise ValueError(\"'table_format' must be one of the following options: 'parquet', 'PARQUET', 'delta', 'DELTA', 'csv', 'CSV'. {0} found.\".format(table_format))\n    \n  if table_path[0] == \"/\":\n    raise ValueError(\"'table_path can't start with the '/' character.\")\n  \n  # Get full_table_path\n  envs = {\"pro\": 3, \"dev\": 6}\n  full_table_path = os.path.join(\"abfss://{0}@{1}satdatalakegen2.dfs.core.windows.net\".format(container, envs[environment_code]), table_path)\n  print(full_table_path)\n  # Read table\n  if table_format in [\"parquet\", \"PARQUET\", \"delta\", \"DELTA\"]:\n    df = (spark.read.format(table_format).load(full_table_path))\n  else:\n    df = (spark.read.format(table_format).load(full_table_path, header=header, sep=sep))\n    \n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9edea556-e647-4a57-8718-981c1dd10711"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_dlake_table(df, table_path, container, mode = \"overwrite\", environment = \"dev\", \n                      table_format = \"delta\", partitioned_by = None, replace_where = None, \n                      header = True, sep = \",\", delete_versions=True):\n  \"\"\"\n  Function to save a table in the Data Lake.\n  \n  Args:\n    - df (DataFrame): Spark DataFrame to be persisted.\n    - table_path (String): Path where the table is saved. Can't start with '/'.\n    - container (String): Container of the Data Lake where the table is stored.\n    - mode (String): Save mode.\n    - environment (String): 'dev' / 'desarrollo' or 'pro' / 'produccion'.\n    - table_format (String): Format of the table (parquet, PARQUET, delta, DELTA, csv, CSV).\n    - partitioned_by (String or List(String)): Columns by which the table should be partitioned.\n    - replace_where (String): Expression to be used in the 'replace_where' option for DELTA tables.\n    - header (Boolean): Boolean that indicates the header option (only for CSV tables).\n    - sep (String): Separator (only for CSV tables).\n    \n  Returns:\n    - Nothing, the table is persisted in the the Data Lake.\n  \"\"\"\n  \n\n  # Check functions inputs\n  if not environment in [\"dev\", \"pro\", \"desarrollo\", \"produccion\"]:\n    raise ValueError(\"'environment' must be one of the following options: 'dev' or 'pro'. {0} found.\".format(table_format))\n  \n  if not table_format in [\"parquet\", \"PARQUET\", \"delta\", \"DELTA\", \"csv\", \"CSV\"]:\n    raise ValueError(\"'table_format' must be one of the following options: 'parquet', 'PARQUET', 'delta', 'DELTA', 'csv', 'CSV'. {0} found.\".format(table_format))\n    \n  if not mode in [\"error\", \"errorifexists\", \"append\", \"overwrite\", \"ignore\"]:\n    raise ValueError(\"'mode' must be one of the following options: 'error', 'errorifexists', 'append', 'overwrite' or 'ignore'. {0} found.\".format(mode))\n  \n  if not replace_where is None and not table_format == \"delta\" :\n    raise ValueError(\"'replace_where' option can only be set if format = 'delta'. {0} found.\".format(replace_where))\n  \n  if partitioned_by is not None and not (type(partitioned_by) is list or type(partitioned_by) is str):\n    raise ValueError(\"'partitioned_by' must be str or list type. {0} found.\".format(type(partitioned_by)))\n    \n  if replace_where is not None and not type(replace_where) is str:\n    raise ValueError(\"'replace_where' must be str type. {0} found.\".format(type(replace_where)))\n    \n  if table_path[0] == \"/\":\n    raise ValueError(\"'table_path can't start with the '/' character.\")\n  \n  # Get full_table_path\n  environment_code = {\"pro\": \"pro\", \"dev\": \"dev\", \"produccion\": \"pro\", \"desarrollo\": \"dev\"}[environment]\n  envs = {\"pro\": 3, \"dev\": 6}\n  full_table_path = os.path.join(\"abfss://{0}@{1}satdatalakegen2.dfs.core.windows.net\".format(container, envs[environment_code]), table_path)\n    \n  # Save table                 \n  if replace_where is None:\n    if partitioned_by is None:\n      if table_format in [\"parquet\", \"PARQUET\", \"delta\", \"DELTA\"]:\n        (df\n         .write\n         .format(table_format)\n         .mode(mode)\n         .save(full_table_path))\n      else:\n        (df\n         .write\n         .format(table_format)\n         .mode(mode)\n         .save(full_table_path, header=header, sep=sep))\n        \n    else:\n      if table_format in [\"parquet\", \"PARQUET\", \"delta\", \"DELTA\"]:\n        (df\n         .write\n         .format(table_format)\n         .mode(mode)\n         .partitionBy(partitioned_by)\n         .save(full_table_path))\n      else:\n        (df\n         .write\n         .format(table_format)\n         .mode(mode)\n         .partitionBy(partitioned_by)\n         .save(full_table_path,  header=header, sep=sep))\n  \n  else:\n    if partitioned_by is None:\n      (df\n       .write\n       .format(table_format)\n       .mode(mode)\n       .option(\"replaceWhere\", replace_where)\n       .save(full_table_path))\n    else:\n      (df\n       .write\n       .format(table_format)\n       .mode(mode)\n       .option(\"replaceWhere\", replace_where)\n       .partitionBy(partitioned_by)\n       .save(full_table_path))\n      \n  if table_format in [\"delta\", \"DELTA\"]:\n    if delete_versions:\n      spark.sql(\"set spark.databricks.delta.retentionDurationCheck.enabled = false\")\n      spark.sql(\"VACUUM delta.`{}` RETAIN 0 HOURS\".format(full_table_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac7c5630-8417-4939-a859-5e0c71f37476"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_previous_day(fecha):\n  \"\"\"\n  Given a date in yyyyMMdd format, returns the previous day\n  \"\"\"\n\n  if fecha[6:8]=='01':\n    fecha_ant = get_lastday_lastmonth(fecha)\n  else:\n    fecha_ant=str(int(fecha)-1)\n  return fecha_ant"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fdc0702-6b5d-4590-a218-14d816ce045e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_today():\n  \"\"\"\n  Returns todays's date in yyyyMMdd format.\n  \"\"\"\n  \n  return datetime.datetime.today().strftime('%Y%m%d')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6f687bc-7070-4a35-a25a-22a1bc18cb49"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_yesterday():\n  \"\"\"\n  Returns yesterday's date in yyyyMMdd format.\n  \"\"\"  \n  \n  return get_previous_day(get_today())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a9dc82d-b986-4526-9c17-834e49b3876b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["LIMITS_COLS_DWH = {\n  \"sk_crpa\": [1, 2.5e8],\n  \"sk_mediador\": [1, 1e6],\n  \"sk_sucursal\": [1, 2e3],\n  \"cod_mediador\": [0, 100000],\n  \"sk_nivcod\": [1, 5000],\n  \"sk_division\": [1, 20],\n  \"sk_territorial\": [1, 120],\n  \"sk_sucursal\": [1, 1300],\n  \"sk_centro\": [1, 4000],\n  \"sk_comercial\": [1, 5000],\n  \"id_familia\": [1, 8e6],   \n  \"sk_siniestro\": [1, 100e6],\n  \"sk_stro_gtia_part\": [1, 120e6],\n  \"sk_cliente\": [1, 100e6],\n  \"antiguedad_cliente\": [-100, 1250],\n  \"sk_encuesta\": [1, 3e6],\n  \"default\": [1, 2147483640],\n  \"sk_apunte\": [1, 2.2e8],\n  \"periodo\": {\"default\": [20011231, int(get_yesterday())], \n              \"FP001\": [20021231, int(get_yesterday())], \n              \"FP002\": [20011231, int(get_yesterday())], \n              \"HP001\": [20181231, int(get_yesterday())], \n              \"FA001\": [20011231, int(get_yesterday())], \n              \"HA001\": [20190228, int(get_yesterday())]}\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e399329-b103-4eb5-a22a-e6a42257a120"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import sys\nfrom pyspark.sql.window import Window as W\n\n\ndef spark_dim_window_filter(tabla, fecha_hub, claves, drop = True, registro_fecha = \"fecha_inicio\"):\n  \"\"\"\n  Filtra registros de un periodo en la tabla de dimensiones sin usar claves subrogadas.\n  Útil cuando tabla hub no está disponible, pero más caro computacionalmente.\n\n  Parameters:\n  -------------\n  fecha_hub : string\n    Fecha en la que se desea realizar la consulta a la tabla \n  claves : lista de strings o string (única clave)\n    Lista de nombre/s del campo/s de clave de negocio para crear las ventanas.\n  compania: dataframe  \n    Tabla de dimensiones\n  drop: Boolean\n    Por defecto elimina duplicados del subconjunto de claves.\n  registro_fecha: string\n    Nombre del campo que contiene la fecha de grabado del registro.\n    Por defecto: \"fecha_inicio.\"\n\n  Returns\n  -------------\n  Un dataframe filtrado para un periodo dado por fecha_hub\n  \n  Ejemplo\n  -------------\n  df001 = spark_dim_window_filter(DF001, \"20201231\", [\"cod_encargo\"] )\n  \"\"\"\n  #Comprobación listado de claves\n  if type(claves) == str:\n    claves = [claves]\n  w_table = W.partitionBy(claves).rowsBetween(-sys.maxsize, sys.maxsize)\n  tabla = (tabla\n           .filter(F.col(registro_fecha) <= fecha_hub)\n           .withColumn(registro_fecha, F.col(registro_fecha).cast(\"int\"))\n           .withColumn(\"max_fecha\", F.max(registro_fecha).over(w_table))\n           .filter(F.col(registro_fecha) == F.col(\"max_fecha\"))\n           .drop(\"max_fecha\")  \n           )  \n  if drop == True:\n    tabla = tabla.dropDuplicates(subset=claves)\n  return tabla"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"266e3e27-a198-4d0f-a86c-8db7f2eeb75f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3340962d-7bca-4651-bf05-70c28118cdb5"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"funciones_generales","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1550552169430463}},"nbformat":4,"nbformat_minor":0}
